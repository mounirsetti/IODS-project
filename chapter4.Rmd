


# Chapter 4: Clustering and classification
### Mounir Ould Setti
### 25/11/2019

## 1. Loading the libraries
```{r echo=FALSE}
library(MASS)
library(tidyverse)
library(corrplot)
library(ggplot2)
```

## 2. Preparing the data

Loading Boston data from the MASS package and exploring it
``` {r}
data("Boston")
str(Boston)
dim(Boston)
```

Boston data is a dataset built in within the MASS package of R. It consists of 506 entries of 14 variables. It was meant initially to explore different factors in relation to the demand for clean air in different towns in Boston area (data from the 70s). [For more information](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

## 3. Graphical overview of the data and summaries of the variables

```{r}
summary(Boston)
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper",cl.pos = "b", tl.pos = "d" , tl.cex = 0.6)

```
All the variables are continuous numerical variables. Some may contain outliers such as crime rate and Charles River dummy variable (chas). The distribution of some is clearly skewed such as crime rate, zn, and chas.
There are many interesting observations to be drawn from the data, it would be out of the scope of this exercise to explore them systematically, but if we take per-capita crime rate by town as an example, we notice that its distribution by properties-age  is skewed to the right indicating that crime rates are higher in towns with the highest proportion of old properties.
Similarly, the distribution of crime rate by blacks distribution is skewed to the right suggesting that towns with the highest concentration of people of black race have higher crime rates.
Some variables also correlate strongly with each other, such as the property tax rate with the index of accessibility to radial highways which correlate positively indicating that the higher the property tax-rate the better the accessibility ot highways. In the opposite, tax-rate have a strong negative correlation with the proportion of old properties (age).

## 4. Working up the dataset
### A- Standardizing the dataset
```{r}
# center and standardize variables
boston_scaled <- as.data.frame(scale(Boston))

# summaries of the scaled variables
summary(boston_scaled)
```

The distribution of the different variables have a common scale now and can be compared to each other. The mean of each variable has also been unified to 0.

### B- Cutting crime rate into a categorical variable

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

### C- Dividing the dataset into training and testing data

``` {r}

# choose randomly 80% of the rows
rows80 <- sample(length(boston_scaled[,1]),  size = length(boston_scaled[,1]) * 0.8)

# create train set
train <- boston_scaled[rows80,]

# create test set 
test <- boston_scaled[-rows80,]


# checking the results
dim(train)
dim(test)
```

## 5. Fitting the Linear discriminant analysis (LDA) and plotting it

``` {r}
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

########
#Biplot#
########

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

## 6. Testing the prediction of crime rate with the LDA model
### A- Saving the crime categories from the test set and removing its original values from the dataset
``` {r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# checking the results (again)
dim(test)
```

### B- Predicting the classes with the LDA model on the test data
``` {r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
The accuracy of the model is `r round(100*(10+19+19+25)/102,1)`%

## 7. Distances, K-means, and clusters
### A- Rescaling Boston dataset
``` {r}
data("Boston")
scaledboston <- as.data.frame(scale(Boston))
summary(scaledboston)
```

### B- Calculating the distances between the observations in the scaled dataset
``` {r}
summary(dist(scaledboston))
```

### C- Attempting K-means with a random number of clusters and visualizing it
``` {r}
# k-means clustering
km <-kmeans(scaledboston, centers = 3)

#plot the scaledoston dataset with clusters
pairs(scaledboston, col = km$cluster)
```
The variables tax, rm and nox seem to affect clustering the most

### D- Finding the optimal number of clusters
``` {r}
##Determining the K
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(scaledboston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
It seems that 2 is the optimal number of clusters as it corresponds to when the value of total WCSS changes radically.

### E- Running the K-means algorithm with the optimum number of clusters and plotting it
``` {r echo=FALSE}
# k-means clustering
km <-kmeans(scaledboston, centers = 2)

# ploting the scaledboston dataset with clusters
pairs(scaledboston, col = km$cluster)

#The following commands would draw the plot using ggpairs but it has too much information in my opinion
##library(GGally)
##ggpairs(scaledboston, mapping = aes(col=as.factor(km$cluster)))
```
The two clusters represent chunks of data with clearly different features. Many variables seem to distinguish them. Perhaps tax is the most distinguishing.

## 8. Bonus

``` {r}
#Performing k-means on the original Boston dataset
data("Boston")
scaledboston2 <- as.data.frame(scale(Boston))
scaledboston2$cluster <-(kmeans(scaledboston2, centers = 4))$cluster
lda.clust <- lda(cluster~., data = scaledboston2)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(scaledboston2$cluster)

# plot the lda results
plot(lda.clust, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.clust, myscale = 1)

```